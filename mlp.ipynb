{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_val = pd.read_csv('data/val.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df = pd.concat([df_train, df_val], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c7f01",
   "metadata": {},
   "source": [
    "# Visualize bar chart with each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['price', 'area', 'street_in_front_of_house', 'width'])\n",
    "total_features = features.columns.to_list()\n",
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_unique_fields = {}\n",
    "for i in total_features:\n",
    "    feature_unique_fields[i] = features[i].unique().tolist()\n",
    "\n",
    "len_features = len(feature_unique_fields)\n",
    "for i in range(len_features):\n",
    "    list(feature_unique_fields.values())[i].sort()\n",
    "feature_unique_value = {}\n",
    "for feature in total_features:\n",
    "    # print(\"Feature: \\n\", feature)\n",
    "    feature_len = len(feature_unique_fields[feature])\n",
    "    # print(\"Len: \", feature_len)\n",
    "    feature_value = feature_unique_fields[feature]\n",
    "    # print(feature_value)\n",
    "    feature_unique_value[feature] = [sum(df[df[feature] == feature_value[i]].price) / df[feature].value_counts()[feature_value[i]] for i in range(feature_len)]\n",
    "    \n",
    "# print(feature_unique_value)\n",
    "feature_unique_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933943ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualized_features = total_features\n",
    "name_of_features = ['floor_number',\n",
    " 'bedroom_number',\n",
    " 'is_dinning_room',\n",
    " 'is_kitchen',\n",
    " 'is_terrace',\n",
    " 'is_car_pack',\n",
    " 'type',\n",
    " 'direction',\n",
    " 'city',\n",
    " 'district']\n",
    "labels = ['Số tầng', 'Số phòng ngủ', 'Có phòng ăn hay không', 'Có phòng bếp hay không', 'Có sân thượng hay không', \n",
    "'Có chỗ để xe hay không','Loại bất động sản','Thành phố', 'Quận/Huyện']\n",
    "titles = ['Biểu đồ thể hiện giá nhà trung bình theo số tầng nhà','Biểu đồ giá nhà trung bình theo số phòng ngủ', 'Biểu đồ giá nhà trung bình theo phòng ăn', \n",
    "'Biểu đồ giá nhà trung bình có và không có bếp', 'Giá nhà trung bình với sân thượng', 'Giá nhà trung bình với chỗ để xe', \n",
    "'Biểu đồ thể hiện giá nhà trung bình theo loại bất động sản', \n",
    "'Biểu đồ thể hiện giá nhà trung bình theo thành phố', 'Biểu đồ thể hiện giá nhà trung bình theo quận/huyện bất động sản']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "count = 0\n",
    "for feature in visualized_features:\n",
    "    N = len(feature_unique_value[feature])\n",
    "    ind = np.arange(N) \n",
    "    all_colors = list(plt.cm.colors.cnames.keys())\n",
    "    random.seed(100)\n",
    "    c = random.choices(all_colors, k=N) \n",
    "    text_value = {}\n",
    "    for i in ind:\n",
    "        text_value[i] = feature_unique_value[feature][i]\n",
    "        \n",
    "    fig = plt.subplots(figsize=(10, 7))\n",
    "    plt.bar(ind, feature_unique_value[feature], color=c)\n",
    "    \n",
    "    for key in text_value:\n",
    "        plt.text(key, text_value[key], float(round(text_value[key], 2)), \n",
    "                horizontalalignment='center', verticalalignment='bottom', \n",
    "                fontdict={'fontweight':500, 'size':12})\n",
    "    \n",
    "    # Decide whether to rotate labels based on number of categories and label length\n",
    "    max_label_length = max([len(str(label)) for label in feature_unique_fields[feature]])\n",
    "    available_width = 10  # Figure width in inches\n",
    "    \n",
    "    # Apply rotation if many values or long labels\n",
    "    if N > 5 or (N * max_label_length > 30):\n",
    "        plt.xticks(ind, list(feature_unique_fields[feature]), rotation=45, ha='right')\n",
    "    else:\n",
    "        plt.xticks(ind, list(feature_unique_fields[feature]))  # No rotation\n",
    "    \n",
    "    plt.xlabel(labels[count])\n",
    "    plt.ylabel(\"Giá (tỷ đồng)\")\n",
    "    plt.title(titles[count], fontsize=22)\n",
    "    plt.tight_layout()\n",
    "    count += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb53377",
   "metadata": {},
   "source": [
    "# Preprocessing to be ready for predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50add25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a93a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numerical = df.select_dtypes(exclude=['object', 'bool']).copy()\n",
    "numerical_cols = features_numerical.columns.tolist()\n",
    "\n",
    "numerical_cols.remove('price')\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical = df.select_dtypes(include=['object', 'bool']).copy()\n",
    "categorical_cols = features_categorical.columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a012ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numerical_transformer, numerical_cols)     \n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['price'].copy()\n",
    "y_val = df_val['price'].copy()\n",
    "\n",
    "df_train = df_train.drop(['price'], axis = 1)\n",
    "df_val = df_val.drop(['price'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_numpy()\n",
    "y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ffd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessor.fit_transform(df_train)\n",
    "X_val = preprocessor.transform(df_val)\n",
    "X_test = preprocessor.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_evaluate(y_true, y_pred):\n",
    "    plt.plot(y_true, y_pred, 'b.')\n",
    "    x = [np.min(y_true), np.max(y_true)]\n",
    "    y = x\n",
    "    plt.plot(x, y, 'r')\n",
    "    plt.title('XGBoost')\n",
    "    plt.xlabel('Reality')\n",
    "    plt.ylabel('Predict')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6b3db",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron\n",
    "So sánh dựa trên các tiêu chí: Số tầng ẩn, Learning rate (Adam), Số lượng epoch, Hàm kích hoạt, Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab8f92",
   "metadata": {},
   "source": [
    "## Số tầng ẩn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54551f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_evaluate(hidden_sizes):\n",
    "    model = FlexibleMLP(X_train_tensor.shape[1], hidden_sizes)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i + batch_size]\n",
    "            y_batch = y_train_tensor[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * len(X_batch)\n",
    "        epoch_train_loss /= len(X_train_tensor) \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_scaled = model(X_val_tensor)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.numpy())\n",
    "        y_val_true = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        \n",
    "        y_train_pred_scaled = model(X_train_tensor)\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.numpy())\n",
    "        y_train_true = scaler_y.inverse_transform(y_train_tensor.numpy())\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "    return model, train_rmse, val_rmse, train_losses, val_losses\n",
    "\n",
    "results = []\n",
    "best_rmse = float('inf')\n",
    "best_model = None\n",
    "best_test_pred = None\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "for n_layers in range(1, 6):\n",
    "    hidden_sizes = [int(256 / (2 ** i)) for i in range(n_layers)]\n",
    "    print(f\"\\nTraining MLP with {n_layers} hidden layer(s), sizes = {hidden_sizes}\")\n",
    "    model, train_rmse, val_rmse, train_losses, val_losses = train_and_evaluate(hidden_sizes)\n",
    "    print(f\"RMSE: {val_rmse:.4f}\")\n",
    "    results.append((n_layers, hidden_sizes, train_rmse, val_rmse))\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_val_losses.append(val_losses)\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_model = model\n",
    "        with torch.no_grad():\n",
    "            y_test_pred_scaled = best_model(X_test_tensor)\n",
    "            y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.numpy())\n",
    "\n",
    "mlp_submit = pd.DataFrame({\n",
    "    'Id': df_test.index,\n",
    "    'TARGET': y_test_pred.flatten()\n",
    "})\n",
    "mlp_submit.to_csv('data/mlp_model_pytorch_best.csv', index=False)\n",
    "\n",
    "print(f\"\\nMô hình tốt nhất: {best_rmse:.4f}\")\n",
    "n_layers_list = [result[0] for result in results]\n",
    "val_rmse_list = [result[3] for result in results]\n",
    "train_rmse_list = [result[2] for result in results]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_layers_list, train_rmse_list, label='Train RMSE')\n",
    "plt.plot(n_layers_list, val_rmse_list, label='Validation RMSE')\n",
    "plt.xlabel('Số tầng ẩn (hidden layers)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('So sánh Train & Validation RMSE theo số tầng ẩn trong MLP')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131804d7",
   "metadata": {},
   "source": [
    "## Learning rate (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, 1))  # Tầng đầu ra\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_evaluate_lr(hidden_sizes, lr):\n",
    "    model = FlexibleMLP(X_train_tensor.shape[1], hidden_sizes)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i + batch_size]\n",
    "            y_batch = y_train_tensor[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_scaled = model(X_val_tensor)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.numpy())\n",
    "        y_val_true = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        \n",
    "        y_train_pred_scaled = model(X_train_tensor)\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.numpy())\n",
    "        y_train_true = scaler_y.inverse_transform(y_train_tensor.numpy())\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "    return train_rmse, val_rmse\n",
    "\n",
    "learning_rates = np.linspace(0.0001, 0.01, 10)\n",
    "train_rmses = []\n",
    "val_rmses = []\n",
    "\n",
    "hidden_sizes = [256, 128, 64]\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate = {lr:.5f}\")\n",
    "    train_rmse, val_rmse = train_and_evaluate_lr(hidden_sizes, lr)\n",
    "    train_rmses.append(train_rmse)\n",
    "    val_rmses.append(val_rmse)\n",
    "    print(f\"Train RMSE = {train_rmse:.4f}, Val RMSE = {val_rmse:.4f}\")\n",
    "    \n",
    "mlp_submit = pd.DataFrame({\n",
    "    'Id': df_test.index,\n",
    "    'TARGET': y_test_pred.flatten()\n",
    "})\n",
    "mlp_submit.to_csv('data/mlp_model_pytorch_best.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(learning_rates, train_rmses, label='Train RMSE')\n",
    "plt.plot(learning_rates, val_rmses, label='Validation RMSE')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (Adam)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Ảnh hưởng của Learning Rate trong thuật toán tối ưu Adam đến RMSE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7760c",
   "metadata": {},
   "source": [
    "## Số lượng epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0038879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_evaluate(hidden_sizes):\n",
    "    model = FlexibleMLP(X_train_tensor.shape[1], hidden_sizes)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i + batch_size]\n",
    "            y_batch = y_train_tensor[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * len(X_batch)\n",
    "        epoch_train_loss /= len(X_train_tensor) \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # if patience_counter >= patience:\n",
    "        #     break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_scaled = model(X_val_tensor)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled.numpy())\n",
    "        y_val_true = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        \n",
    "        y_train_pred_scaled = model(X_train_tensor)\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.numpy())\n",
    "        y_train_true = scaler_y.inverse_transform(y_train_tensor.numpy())\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "    return model, train_rmse, val_rmse, train_losses, val_losses\n",
    "\n",
    "results = []\n",
    "best_rmse = float('inf')\n",
    "best_model = None\n",
    "best_test_pred = None\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "hidden_sizes = [256, 128, 64]\n",
    "print(f\"\\nTraining MLP with 3 hidden layer(s), sizes = {hidden_sizes}\")\n",
    "model, train_rmse, val_rmse, train_losses, val_losses = train_and_evaluate(hidden_sizes)\n",
    "print(f\"RMSE: {val_rmse:.4f}\")\n",
    "results.append((3, hidden_sizes, train_rmse, val_rmse))\n",
    "all_train_losses.append(train_losses)\n",
    "all_val_losses.append(val_losses)\n",
    "if val_rmse < best_rmse:\n",
    "    best_rmse = val_rmse\n",
    "    best_model = model\n",
    "    with torch.no_grad():\n",
    "        y_test_pred_scaled = best_model(X_test_tensor)\n",
    "        y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.numpy())\n",
    "\n",
    "mlp_submit = pd.DataFrame({\n",
    "    'Id': df_test.index,\n",
    "    'TARGET': y_test_pred.flatten()\n",
    "})\n",
    "mlp_submit.to_csv('data/mlp_model_pytorch_best.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd51e60",
   "metadata": {},
   "source": [
    "## Hàm kích hoạt\n",
    "Bao gồm ReLU, LeakyRelu với hệ số 0.01, ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation_fn):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_evaluate_activation(hidden_sizes, activation_fn):\n",
    "    model = FlexibleMLP(X_train_tensor.shape[1], hidden_sizes, activation_fn)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i + batch_size]\n",
    "            y_batch = y_train_tensor[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val_tensor)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred.numpy())\n",
    "        y_val_true = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "        y_train_pred = model(X_train_tensor)\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred.numpy())\n",
    "        y_train_true = scaler_y.inverse_transform(y_train_tensor.numpy())\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "    return train_rmse, val_rmse\n",
    "\n",
    "activation_functions = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"LeakyReLU\": lambda: nn.LeakyReLU(negative_slope=0.01),\n",
    "    \"ELU\": nn.ELU\n",
    "}\n",
    "\n",
    "train_rmses = []\n",
    "val_rmses = []\n",
    "\n",
    "for name, act_fn in activation_functions.items():\n",
    "    print(f\"\\nTraining with activation function = {name}\")\n",
    "    hidden_sizes = [256, 128, 64]\n",
    "    train_rmse, val_rmse = train_and_evaluate_activation(hidden_sizes, act_fn)\n",
    "    train_rmses.append(train_rmse)\n",
    "    val_rmses.append(val_rmse)\n",
    "    print(f\"Train RMSE = {train_rmse:.4f}, Val RMSE = {val_rmse:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "x = np.arange(len(activation_functions))\n",
    "plt.plot(x, train_rmses, marker = 'o', label='Train RMSE')\n",
    "plt.plot(x, val_rmses, marker = 'o', label='Validation RMSE')\n",
    "plt.xticks(x, list(activation_functions.keys()))\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE theo Hàm Kích Hoạt (Activation Function)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a8b06",
   "metadata": {},
   "source": [
    "## Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, dropout_rate=0.3):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_evaluate_dropout(hidden_sizes, dropout_rate):\n",
    "    model = FlexibleMLP(X_train_tensor.shape[1], hidden_sizes, dropout_rate)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i + batch_size]\n",
    "            y_batch = y_train_tensor[i:i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val_tensor)\n",
    "        y_val_pred = scaler_y.inverse_transform(y_val_pred.numpy())\n",
    "        y_val_true = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "\n",
    "        y_train_pred = model(X_train_tensor)\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred.numpy())\n",
    "        y_train_true = scaler_y.inverse_transform(y_train_tensor.numpy())\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "\n",
    "    return train_rmse, val_rmse\n",
    "\n",
    "dropout_rates = np.linspace(0.1, 0.5, 10)\n",
    "train_rmses = []\n",
    "val_rmses = []\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    print(f\"\\nTraining with dropout rate = {dropout:.2f}\")\n",
    "    hidden_sizes = [256, 128, 64]\n",
    "    train_rmse, val_rmse = train_and_evaluate_dropout(hidden_sizes, dropout)\n",
    "    train_rmses.append(train_rmse)\n",
    "    val_rmses.append(val_rmse)\n",
    "    print(f\"Train RMSE = {train_rmse:.4f}, Val RMSE = {val_rmse:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(dropout_rates, train_rmses, label='Train RMSE')\n",
    "plt.plot(dropout_rates, val_rmses, label='Validation RMSE')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Ảnh hưởng của Dropout Rate đến RMSE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
